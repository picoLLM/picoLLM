# These use the nginx rp to localhost to commuicate with ports 8000 and 11434
OLLAMA_HOST="http://nginx"
VLLM_HOST="http://nginx/vllm/v1"

OPENAI_API_KEY=your-api-key
ANTHROPIC_API_KEY=your-api-key


DENSE_MODEL="mixedbread-ai/mxbai-embed-large-v1"
# Theres a few models that can be used here, including qdrant bm42 fastembed
SPARSE_MODEL_QUERY="naver/efficient-splade-VI-BT-large-query"
SPARSE_MODEL_DOCS="naver/efficient-splade-VI-BT-large-doc"
SENTENCE_TRANSFORMER_BACKEND=onnx
# SENTENCE_TRANSFORMER_ONNX_PATH=onnx/model_path_in_hf.onnx

# If you want to load an additional rerank model uncomment and set this
# Default behavior utilizes recirprocal ranking fusion and does not need a model. Results are roughly the exact same even adding a model on top of the RRF
# RERANK_MODEL="mixedbread-ai/mxbai-rerank-large-v1"

QDRANT_URI="http://qdrant"

# postgres connection string
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
POSTGRES_DB=picollm
DATABASE_URL=postgresql://postgres:password@0.0.0.0:5434/picollm

# Use this if you want to use any private embedding models, tokenizers, or datasets
# HF_TOKEN=your-hf-token 
# Defaults to follow your global HF_HOME location
# HF_HOME=your-default-hf-cache-location

### optional for frontend. you can leave these default unless you want to deploy the frontend separately
VITE_DEFAULT_MODEL=gemma3:4b
VITE_PAGE_SIZE=20
VITE_ENABLE_CACHING=true
VITE_API_URL=http://fastapi:8080
VITE_BACKEND_URL=http://fastapi:8080
VITE_MODEL_BASE_URL=http://fastapi:8080
VITE_QDRANT_URI=http://0.0.0.0:6333
VITE_OLLAMA_URI=http://nginx/v1
